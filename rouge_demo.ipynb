{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rouge\n",
    "import pandas as pd\n",
    "\n",
    "def scoring(bertdf,userdf):\n",
    "    \n",
    "    #1.1 clean up bert df, extract the first row, and reset index\n",
    "    bertsum = bertdf.iloc[0:1]\n",
    "    bertsum =bertsum.to_string(header=False,index=False,index_names=False)\n",
    "    bertsum= [int(s) for s in bertsum.split(',')]\n",
    "    \n",
    "    \n",
    "    bertdf=bertdf.drop(0).reset_index(drop=True)\n",
    "    \n",
    "    #1.2 clean up user df, extract the first row, and reset index\n",
    "    usersum = userdf.iloc[0:1]\n",
    "    usersum =usersum.to_string(header=False,index=False,index_names=False)\n",
    "    usersum= [int(s) for s in usersum.split(',')]\n",
    "    \n",
    "    userdf=userdf.drop(0).reset_index(drop=True)    \n",
    "    \n",
    "    #2.1 convert to list\n",
    "    berttitle= bertdf.columns[0]\n",
    "    cleanedbert =bertdf.iloc[bertsum , : ]\n",
    "    \n",
    "    bertdf =cleanedbert[berttitle].tolist()\n",
    "    bert = ' '.join([str(elem) for elem in bertdf]) \n",
    "    \n",
    "    #2.2 convert to list\n",
    "    usertitle= userdf.columns[0]\n",
    "    cleaneduser =userdf.iloc[usersum , : ]\n",
    "    \n",
    "    userdf =cleaneduser[usertitle].tolist()\n",
    "    user = ' '.join([str(elem) for elem in userdf]) \n",
    "    \n",
    "    \n",
    "    #scoring, we use rouge-l (ROUGE-L: Longest Common Subsequence based statistics, takes sentences into account) \n",
    "    from rouge import Rouge\n",
    "    \n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(bert, user)\n",
    "    f_score =scores[0][\"rouge-l\"][\"f\"]\n",
    "    precision =scores[0][\"rouge-l\"][\"p\"]\n",
    "    recall =scores[0][\"rouge-l\"][\"r\"]\n",
    "    \n",
    "    #print(f1,precision,recall)\n",
    "\n",
    "    print(\"BERT: \"+str(bertsum) +\"\\nUSER: \"+str(usersum))\n",
    "\n",
    "    \n",
    "    print(\"\\n\\nROUGE scoring:\\n\\n\"+\n",
    "          \"Precision is :\"+\"{:.2%}\".format(precision)+\n",
    "          \"\\nRecall is :\"+\"{:.2%}\".format(recall)+\n",
    "          \"\\nF Score is :\"+\"{:.2%}\".format(f_score))\n",
    "    \n",
    "    print(\"\\nPrecision: how much BERT summary exceeds human summary, (if less than 100% means user removed sentences)\\n\"\n",
    "          \"Recall: how much BERT summary explains the human summary, (if less than 100% means user added sentences)\\n\"\n",
    "          \"F Score: aggregation of BERT performance,(if 100% means perfect match)\")\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY PERFECT: If summariser is same as human: \n",
      "\n",
      "BERT: [0, 1, 7, 15, 20, 25, 32, 35, 36, 43]\n",
      "USER: [0, 1, 7, 15, 20, 25, 32, 35, 36, 43]\n",
      "\n",
      "\n",
      "ROUGE scoring:\n",
      "\n",
      "Precision is :100.00%\n",
      "Recall is :100.00%\n",
      "F Score is :100.00%\n",
      "\n",
      "Precision: how much BERT summary exceeds human summary, (if less than 100% means user removed sentences)\n",
      "Recall: how much BERT summary explains the human summary, (if less than 100% means user added sentences)\n",
      "F Score: aggregation of BERT performance,(if 100% means perfect match)\n"
     ]
    }
   ],
   "source": [
    "#input 2 csv file and convert it to dataframe\n",
    "bert = pd.read_csv(r\"C:\\Users\\User\\Desktop\\TIPP\\11 NVidia project\\data\\testbert.csv\")\n",
    "user = pd.read_csv(r\"C:\\Users\\User\\Desktop\\TIPP\\11 NVidia project\\data\\testuser_same.csv\")\n",
    "\n",
    "print(\"SUMMARY PERFECT: If summariser is same as human: \\n\")\n",
    "#scoring (machine, human) - this order is important\n",
    "scoring(bert,user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARISER NOT PERFECT: BERT has less sentences: \n",
      "\n",
      "BERT: [0, 1, 7, 15, 20, 25, 32, 35, 36, 43]\n",
      "USER: [0, 1, 7, 15, 20, 25, 32, 35, 36, 42, 43, 44]\n",
      "\n",
      "\n",
      "ROUGE scoring:\n",
      "\n",
      "Precision is :100.00%\n",
      "Recall is :81.36%\n",
      "F Score is :89.72%\n",
      "\n",
      "Precision: how much BERT summary exceeds human summary, (if less than 100% means user removed sentences)\n",
      "Recall: how much BERT summary explains the human summary, (if less than 100% means user added sentences)\n",
      "F Score: aggregation of BERT performance,(if 100% means perfect match)\n"
     ]
    }
   ],
   "source": [
    "#input 2 csv file and convert it to dataframe\n",
    "bert = pd.read_csv(r\"C:\\Users\\User\\Desktop\\TIPP\\11 NVidia project\\data\\testbert.csv\")\n",
    "user = pd.read_csv(r\"C:\\Users\\User\\Desktop\\TIPP\\11 NVidia project\\data\\testuser_add.csv\")\n",
    "\n",
    "print(\"SUMMARISER NOT PERFECT: BERT has less sentences: \\n\")\n",
    "#scoring (machine, human) - this order is important\n",
    "scoring(bert,user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARISER NOT PERFECT: BERT has more sentences: \n",
      "\n",
      "BERT: [0, 1, 7, 15, 20, 25, 32, 35, 36, 43]\n",
      "USER: [7, 15, 20, 25, 32, 35, 36, 43]\n",
      "\n",
      "\n",
      "ROUGE scoring:\n",
      "\n",
      "Precision is :77.09%\n",
      "Recall is :100.00%\n",
      "F Score is :87.07%\n",
      "\n",
      "Precision: how much BERT summary exceeds human summary, (if less than 100% means user removed sentences)\n",
      "Recall: how much BERT summary explains the human summary, (if less than 100% means user added sentences)\n",
      "F Score: aggregation of BERT performance,(if 100% means perfect match)\n"
     ]
    }
   ],
   "source": [
    "#input 2 csv file and convert it to dataframe\n",
    "bert = pd.read_csv(r\"C:\\Users\\User\\Desktop\\TIPP\\11 NVidia project\\data\\testbert.csv\")\n",
    "user = pd.read_csv(r\"C:\\Users\\User\\Desktop\\TIPP\\11 NVidia project\\data\\testuser_minus.csv\")\n",
    "\n",
    "print(\"SUMMARISER NOT PERFECT: BERT has more sentences: \\n\")\n",
    "#scoring (machine, human) - this order is important\n",
    "scoring(bert,user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
